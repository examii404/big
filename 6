driver.java

package employee;
import java.io.*;
import java.util.*;
import org.apache.hadoop.mapred.*;
import org.apache.hadoop.io.*;
import org.apache.hadoop.fs.Path;

public class driver
{
    public static void main(String args[]) throws IOException
    {
   	 JobConf conf=new JobConf(driver.class);
   	 conf.setMapperClass(mapper.class);
   	 conf.setReducerClass(reducer.class);
   	 conf.setOutputKeyClass(Text.class);
   	 conf.setOutputValueClass(DoubleWritable.class);
   	 FileInputFormat.addInputPath(conf,new Path(args[0]));
   	 FileOutputFormat.setOutputPath(conf,new Path(args[1]));
   	 JobClient.runJob(conf);
    }
}

mapper.java

package employee;
import java.io.*;
import java.util.*;
import org.apache.hadoop.mapred.*;
import org.apache.hadoop.io.*;
class mapper extends MapReduceBase implements Mapper<LongWritable , Text , Text , DoubleWritable> {

    public void map(LongWritable key, Text value, OutputCollector<Text,DoubleWritable> output ,Reporter r) throws IOException
    {
   	 String[] line=value.toString().split("\\t");
   	 	 salary=Double.parseDouble(line[8]);
   	 output.collect(new Text(line[3]), new DoubleWritable(salary));
    }
    
}

reducer.java

package employee;
import java.io.*;
import java.util.*;
import org.apache.hadoop.mapred.*;
import org.apache.hadoop.io.*;
class reducer extends MapReduceBase implements Reducer<Text,DoubleWritable,Text,DoubleWritable> {
public void reduce(Text key,Iterator<DoubleWritable> value , OutputCollector<Text,DoubleWritable> output ,Reporter r) throws IOException
    {
   	 int count=0;
   	 Double sum=0.0;
   	 while(value.hasNext()){
   		 sum+=value.next().get();
   		 count+=1;
   	 }
   	 output.collect(new Text(key+" Average"), new DoubleWritable(sum/count));
   	 output.collect(new Text(key+" Count"), new DoubleWritable(count));
    }
}

